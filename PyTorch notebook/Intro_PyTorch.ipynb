{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialitation of tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a tensor from data\n",
    "A tensor can be created directly from data or from an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing numpy and torch\n",
    "import torch\n",
    "import numpy as np\n",
    "# Creating data \n",
    "data = [[1,2],[3,4]] # \n",
    "# Tensor from data\n",
    "x_data = torch.tensor(data)\n",
    "# Creating array\n",
    "np_array = np.array(data)\n",
    "# Tensor from array\n",
    "x_np = torch.tensor(np_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tensor can also be created using the properties of some data or an array using \"like\" commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor of ones with size and data type property copied from x_data\n",
    "x_ones = torch.ones_like(x_data)\n",
    "# Tensor with random values with size property copied from x_data, data type property is overruled\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With random or constant values\n",
    "A tensor can also be created using random values or constant values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define shape of tensor\n",
    "shape = (2,3,)\n",
    "# Generate random tensor\n",
    "rand_tensor = torch.rand(shape)\n",
    "# Tensor with ones\n",
    "ones_tensor = torch.ones(shape)\n",
    "# Zero tensor\n",
    "zeros_tensor = torch.zeros(shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor properties\n",
    "A tensor has three properties besides its values which is shape, data type and associated device (CPU or GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor: torch.Size([3, 4])\n",
      "Datatype of tensor: torch.float32\n",
      "Device tensor is stored on: cpu\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(3,4)\n",
    "\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor operations\n",
    "Many tensor operations exist (over 100, https://pytorch.org/docs/stable/torch.html)\n",
    "Most PyTorch operations work just as NumPy operations.\n",
    "To speed up computation the tensors can be transferred to GPU if one is available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transferring to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We move our tensor to the GPU if available\n",
    "if torch.cuda.is_available():\n",
    "  tensor = tensor.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some simple operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating tensor\n",
    "tensor = torch.ones(4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# Indexing and assignment\n",
    "tensor[:,1] = 0\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# Concattenation \n",
    "t1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor = tensor.mul(2)\n",
      "tensor([[2., 0., 2., 2.],\n",
      "        [2., 0., 2., 2.],\n",
      "        [2., 0., 2., 2.],\n",
      "        [2., 0., 2., 2.]])\n",
      "tensor3 = tensor2.mul(tensor2)\n",
      "tensor([[4., 0., 4., 4.],\n",
      "        [4., 0., 4., 4.],\n",
      "        [4., 0., 4., 4.],\n",
      "        [4., 0., 4., 4.]])\n"
     ]
    }
   ],
   "source": [
    "# Elementwise multiplication\n",
    "tensor2 = tensor.mul(2)\n",
    "print(\"tensor = tensor.mul(2)\")\n",
    "print(tensor2)\n",
    "\n",
    "tensor3 = tensor2.mul(tensor2)\n",
    "print(\"tensor3 = tensor2.mul(tensor2)\")\n",
    "print(tensor3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor = tensor.mul(tensor)\n",
      "tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "# Matrix multiplication\n",
    "tensor4 = tensor.matmul(tensor.T)\n",
    "print(\"tensor = tensor.mul(tensor)\")\n",
    "print(tensor4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensors and Numpy\n",
    "Tensors and NumPy arrays can share the same memory location and thus changing one will change the other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy to Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([1., 1., 1., 1., 1.])\n",
      "n: [1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Create tensor from data\n",
    "t = torch.ones(5)\n",
    "print(f\"t: {t}\")\n",
    "# Coverting to numpy array\n",
    "n = t.numpy()\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([2., 2., 2., 2., 2.])\n",
      "n: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "# Adding 1 to tensor also affect NumPy array\n",
    "t.add_(1)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor to NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create NumPy array\n",
    "n = np.ones(5)\n",
    "t = torch.from_numpy(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n",
      "n: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "# Adding 1 to NumPy array affects tensor\n",
    "np.add(n, 1, out=n)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "source": [
    "# Training a simple neural network\n",
    "'torch.autograd' is PyTorch’s automatic differentiation engine that powers neural network training.\n",
    "\n",
    "For this example, we load a pretrained resnet18 model from torchvision. We create a random data tensor to represent a single image with 3 channels, and height & width of 64, and its corresponding label initialized to some random values.\n",
    "\n",
    "Training a NN happens in two steps:\n",
    "\n",
    "* Forward Propagation: In forward prop, the NN makes its best guess about the correct output. It runs the input data through each of its functions to make this guess.\n",
    "\n",
    "* Backward Propagation: In backprop, the NN adjusts its parameters proportionate to the error in its guess. It does this by traversing backwards from the output, collecting the derivatives of the error with respect to the parameters of the functions (gradients), and optimizing the parameters using gradient descent. \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision # import PyTorch packages\n",
    "model = torchvision.models.resnet18(pretrained=True) # import resnet18 model\n",
    "data = torch.rand(1, 3, 64, 64) # generate random data\n",
    "labels = torch.rand(1, 1000) # generate random labels"
   ]
  },
  {
   "source": [
    "We run the input data through the model through each of its layers to make a prediction. This is the forward pass."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model(data) # forward pass"
   ]
  },
  {
   "source": [
    "We use the model’s prediction and the corresponding label to calculate the error (loss). \n",
    "\n",
    "The next step is to backpropagate this error through the network. Backward propagation is kicked off when we call .backward() on the error tensor. \n",
    "Autograd then calculates and stores the gradients for each model parameter in the parameter’s .grad attribute."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = (prediction - labels).sum() # Calculating loss (prediction error)\n",
    "loss.backward() # backward pass"
   ]
  },
  {
   "source": [
    "Now we optimize over the model variables using gradient descent to mminimise prediction error.\n",
    "\n",
    "In this example we use Stochastic Gradient Descent (SGD) with momentum\n",
    "To do this we need to set two \"hyperparameters\", namely learning rate (step size) and momentum.\n",
    "\n",
    "The learning rate determines how quickly the model adapts to the problem. A small learning rate means a smaller change in network weights each training iteration whereas a bigger laerning rate means bigger changes in network weights. Choosing the learning rate is important to ensure that the model does not converge to suboptimal solutions (too big) as well as not getting stuck (too small). Here we choose 0.01\n",
    "\n",
    "The momentum is a hyperparameter which determines the number of points that is used for exponetial weighting. Specifically we average over the last 1/(1 - momentum) points. A good value is often 0.9 i.e. averaging over last 10 points. \n",
    "\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9) # loading SGD optimizer on model parameters"
   ]
  },
  {
   "source": [
    "We use the .step() to initiate gradient descent. The optimizer adjusts each parameter by its gradient stored in .grad."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim.step() #gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}